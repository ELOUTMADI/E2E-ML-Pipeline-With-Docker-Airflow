About the Project :
𝗛𝗲𝗿𝗲 𝗮𝗿𝗲 𝘁𝗵𝗲 𝘀𝘁𝗲𝗽𝘀 𝗼𝗳 𝘁𝗵𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲 :

➡️ Get daily batches from external sources and save them for future use.

➡️ Split the data into train and test sets and save them for reference.

➡️ Perform k-fold cross-validation training to tune hyperparameters and choose the best set of parameters.

➡️ Evaluate the performance on the test set.

➡️ Store experimental results (best parameters, training conditions, test set performance)

➡️ The best estimator fit for all data (train and test sets). Save the obtained model for future use.


We will implement this pipeline using Apache Airflow, a popular open-source orchestrator that allows for programmatically building, scheduling, and monitoring workflows. We installed it via #dockercompose and configured it to run in our local environment.
